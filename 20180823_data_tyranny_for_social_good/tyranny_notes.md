# The Tyranny of Data? The Bright and Dark Sides of Data-Driven Decision-Making for Social Good

## Summary Notes

Data tyranny adapted from Easterly's tyranny of experts:

- expert justifications considered objective
- beneficiaries are uninformed about process
- experts act coercively without systems for checking them and are allowed to validate their own systems against their own "objective" rules

Rise of data for good:
- human decision-making is riddled with bias
- algorithms can be used for processes too complex for human understanding
- already in use in policing, healthcare, and economics

Problems:

- Can infer lots of missing private information (identity, sexual orientation, ethnicity, mh diagnosis)
- information asymmetry deepens power differentials between people and data holders
- types of lack of transparency
    - intentional opacity: to protect IP
    - illiterate opacity: people lack the technical understanding
    - intrinsic opacity: algorithms are just hard to interpret
- social exclusion and discrimination
    - disparate impact if data are not weighted properly (e.g. indirect discrimination by overweighting zip codes)
    - creating an algorithm in the first place (e.g., direct discrimination through disparate treatment)
    - misuse of models in inappropriate contexts
    - biased training data used to justify and validate algorithms
- lose access to or be denied benefits on the basis of others' behavior
- lack of transparency means that detecting the source of errors (e.g., incorrect entity resolution, bad data values) is much harder
- siloed data prohibits users from accessing and controling their own data

human-centric requirements for social good algorithms:

- grant people "personal data vaults" where they can own and control their personal data
- improve transparency and accountability
    - tools to identify and rectify bias
    - explanation tools
    - send the code to the data (instead of vice versa; OPAL)
- "living labs" to experiment with data
    - communities of volunteers willing to participate in policy experiments

Organizations working on transparent algorithms:

- Data Transparency Lab
- DARPA Explainable Artificial Intelligence (XAI) project
- New York Universityâ€™s Information Law Institute, such as Helen Nissenbaum and Solon Barocas
- Microsoft Research, such as Kate Crawford and Tarleton Gillespie
- Open Algorithms (OPAL) project (Orange, the MIT Media Lab, Data-Pop Alliance, Imperial College London, and the World Economic Forum)

## Questions

- How do we properly consent people to the use of their data for thousands of applications, many not yet invented? How do we consent people in a way that they will attend to and understand that isn't purely compliance-based like IRB consent forms or dense like TOS?
- What education is required to make people informed about data? Who is responsible for providing that education? Where will it be delivered? Can we ethically develop algorithms for populations that are unlikely or unable to receive that education?
- How can people make informed decisions about their data? Given the millions of data points they produce in a day, how can they view, explore, and understand their data sufficiently to give *informed* consent?
- How could we make data users accountable to human-centric requirements? What incentives are there for following those requirements? What organizations (goverments) are responsible for policing users? What powers could they have for enforcement? How would they discover data misuse?
- For many of the applications DSaPP works on, consent requirements would either delay development and implementation of algorithms for years (e.g., need to wait five years for enough consents from people entering jail, homeless services, etc.) or be highly ambiguous (who needs to consent to data use for multiple housing inspections? building managers? tenants? building owners? what happens when they turnover?). Is this simply a cost we should pay? Are there some cases where consent is not required?
